{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import emd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_PATH = os.path.join(\"..\", \"animal_sounds\")\n",
    "DATASET_AUDIO_PATH = os.path.join(\"..\", \"animal_sounds_clips\")\n",
    "# The sampling rate to use.\n",
    "# This is the one used in all of the audio samples.\n",
    "# This will also be the output size of the audio wave samples\n",
    "# (since all samples are of 1 second long)\n",
    "SAMPLING_RATE = 16000\n",
    "# percentage of recordings used for validation\n",
    "TEST_RATIO = 0.05\n",
    "CLIP_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    if not os.path.exists(DATASET_AUDIO_PATH):\n",
    "        os.makedirs(DATASET_AUDIO_PATH)\n",
    "\n",
    "    for class_name in os.listdir(RAW_DATASET_PATH):\n",
    "        before = 0\n",
    "        after = 0\n",
    "        files = os.listdir(os.path.join(RAW_DATASET_PATH, class_name))\n",
    "        for file in files:\n",
    "            if before < TEST_RATIO * len(files):\n",
    "                target_path = os.path.join(DATASET_AUDIO_PATH, class_name[:-2]+\"_valid\")\n",
    "            else:\n",
    "                target_path = os.path.join(DATASET_AUDIO_PATH, class_name[:-2])\n",
    "            if not os.path.exists(target_path):\n",
    "                os.makedirs(target_path)\n",
    "            p = os.path.join(RAW_DATASET_PATH, class_name, file)\n",
    "            try:\n",
    "                audio = AudioSegment.from_wav(p)\n",
    "                audio = audio.set_frame_rate(SAMPLING_RATE)\n",
    "            except:\n",
    "                print(f\"Could not load file {p}. Skipping.\")\n",
    "                continue\n",
    "            before += 1\n",
    "            step = math.floor(CLIP_LEN / 2.5) if class_name.startswith(\"kidney\") or class_name.startswith(\"muscle\") else CLIP_LEN // 2\n",
    "            for t in range(3000, len(audio)-CLIP_LEN, step):\n",
    "                after += 1\n",
    "                new_audio = audio[t:t+CLIP_LEN]\n",
    "                split = os.path.splitext(file)\n",
    "                filename = split[0] + f\"_{after}_{class_name}\" + split[1]\n",
    "                new_audio.export(os.path.join(target_path, filename),\n",
    "                                format=\"wav\", parameters=[\"-sample_fmt\", \"s16\"])\n",
    "        print(f\"{class_name}: Split {before} tracks into {after} clips of length {CLIP_LEN/1000}s\")\n",
    "\n",
    "prepare_data() # this needs to be run just once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_and_labels_to_dataset(audio_paths, labels, preprocessing_func):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    audios = [path_to_audio(path) for path in audio_paths]\n",
    "    audios = [preprocessing_func(audio) for audio in audios]\n",
    "    audio_ds = tf.data.Dataset.from_tensor_slices(audios)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "    return audio.numpy()\n",
    "\n",
    "# TODO\n",
    "# def normalize(x, axis=0):\n",
    "#     return minmax_scale(x, axis=axis)\n",
    "\n",
    "def join(array_1, array_2):\n",
    "    return np.concatenate((array_1, array_2))\n",
    "\n",
    "def audio_to_fft(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
    "    # we need to squeeze the dimensions and then expand them again\n",
    "    # after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT\n",
    "    # which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
    "\n",
    "# TODO: below functions can be used as replacement/addition to audio_to_fft. Call them in analogous way\n",
    "def audio_to_mfccs(audio):\n",
    "    audio = audio.reshape(audio.shape[0],)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=SAMPLING_RATE, n_fft=len(audio) // 2).flatten()\n",
    "    mfccs = mfccs.reshape(1, mfccs.shape[0])\n",
    "    return mfccs\n",
    "\n",
    "def audio_to_centroids(audio):\n",
    "    audio = audio.reshape(audio.shape[0],)\n",
    "    audio_length = len(audio)\n",
    "    centroids = librosa.feature.spectral_centroid(y=audio, sr=SAMPLING_RATE, n_fft=audio_length, hop_length=audio_length // 64)[0]\n",
    "    centroids = centroids.reshape(1, centroids.shape[0])\n",
    "    return centroids\n",
    "    # return normalize(centroids) # normalization does not make sense at interval\n",
    "    \n",
    "def audio_to_imfs(audio, to_take=8):\n",
    "    imfs = emd.sift.sift(audio)\n",
    "    return imfs[:, :to_take]\n",
    "\n",
    "def preprocessing_pure(audio):\n",
    "    return audio\n",
    "\n",
    "def preprocessing_only_imfs(audio):\n",
    "    return audio_to_imfs(audio)\n",
    "\n",
    "def preprocessing_imfs_with_hilbert_huang(audio):\n",
    "    imfs = audio_to_imfs(audio)\n",
    "    IP, IF, IA = emd.spectra.frequency_transform(imfs, SAMPLING_RATE, 'hilbert')\n",
    "    return join(join(imfs, IP), join(IF, IA))\n",
    "\n",
    "def preprocessing_only_mfccs(audio):\n",
    "    return audio_to_mfccs(audio)\n",
    "\n",
    "def preprocessing_only_centroids(audio):\n",
    "    return audio_to_centroids(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_SEED = 5123\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100 # TODO\n",
    "PREPROCESSING_FUNC = preprocessing_only_mfccs\n",
    "TO_TAKE_COUNT = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of audio file paths along with their corresponding labels\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "print(\"Our class names: {}\".format(class_names,))\n",
    "\n",
    "train_audio_paths = []\n",
    "valid_audio_paths = []\n",
    "train_labels = []\n",
    "valid_labels = []\n",
    "for label, name in enumerate(class_names):\n",
    "    # print(\"Processing material {}\".format(name,))\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.lower().endswith(\".wav\")\n",
    "    ]\n",
    "    label = label // 2 # coz every dir has a _valid copy\n",
    "    if name.endswith(\"_valid\"):\n",
    "        valid_audio_paths += speaker_sample_paths\n",
    "        valid_labels += [label] * len(speaker_sample_paths)\n",
    "    else:\n",
    "        train_audio_paths += speaker_sample_paths\n",
    "        train_labels += [label] * len(speaker_sample_paths)\n",
    "    \n",
    "print(\n",
    "    \"Found {} files belonging to {} classes.\".format(len(train_audio_paths) + len(valid_audio_paths), len(class_names)//2)\n",
    ")\n",
    "\n",
    "# Shuffle\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(train_audio_paths)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(valid_audio_paths)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(train_labels)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(valid_labels)\n",
    "\n",
    "print(\"Using {} files for training.\".format(len(train_labels)))\n",
    "print(\"Using {} files for validation.\".format(len(valid_labels)))\n",
    "\n",
    "# Create 2 datasets, one for training and the other for validation\n",
    "\n",
    "start = time.time()\n",
    "train_audio_paths = train_audio_paths[:TO_TAKE_COUNT]\n",
    "valid_audio_paths = valid_audio_paths[:TO_TAKE_COUNT]\n",
    "train_labels = train_labels[:TO_TAKE_COUNT]\n",
    "valid_labels = valid_labels[:TO_TAKE_COUNT]\n",
    "\n",
    "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels, PREPROCESSING_FUNC)\n",
    "# train_ds = train_ds.map(lambda x, y: (tf.reshape(x, (1, x.shape[0])), y))\n",
    "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(f\"finished preparing training dataset : {elapsed:.2f}\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels, PREPROCESSING_FUNC)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = list(train_ds.take(1))[0][0].shape[1:]\n",
    "valid_shape = list(valid_ds.take(1))[0][0].shape[1:] # just to check everything is ok\n",
    "assert INPUT_SHAPE == valid_shape\n",
    "print(INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
    "    s = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    for _ in range(conv_num - 1):\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=1, strides=2)(x)\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = residual_block(inputs, 16, 2)\n",
    "    x = residual_block(x, 32, 2)\n",
    "    x = residual_block(x, 64, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "\n",
    "    x = keras.layers.AveragePooling1D(pool_size=1, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = build_model(INPUT_SHAPE, len(class_names))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model using Adam's default learning rate\n",
    "model.compile(\n",
    "    optimizer=\"Adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Add callbacks:\n",
    "# 'EarlyStopping' to stop training when the model is not enhancing anymore\n",
    "# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n",
    "model_save_filename = \"my_best_model.h5\"\n",
    "\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)\n",
    "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history is not None:\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = paths_and_labels_to_dataset(train_audio_paths + valid_audio_paths, train_labels + valid_labels, PREPROCESSING_FUNC)\n",
    "test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "actual_class_names = [n for n in class_names if not n.endswith(\"_valid\")]\n",
    "\n",
    "confusion = np.zeros((len(actual_class_names),)*2, dtype=np.float32)\n",
    "counter = 0\n",
    "for audios, labels in test_ds.take(1000):\n",
    "    # Get the signal FFT\n",
    "    ffts = audio_to_fft(audios)\n",
    "    # Predict\n",
    "    y_pred = model.predict(ffts)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    for idx, pred in enumerate(y_pred):\n",
    "        counter += 1\n",
    "        confusion[labels[idx], pred] += 1\n",
    "\n",
    "print(counter)\n",
    "\n",
    "\n",
    "for idx, label in enumerate(actual_class_names):\n",
    "    confusion[idx, :] /= np.sum(confusion[idx, :])\n",
    "    print(f\"{idx} - {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "labels = [x.split(\"_\")[0] for x in actual_class_names]\n",
    "# labels = [\"ribs\", \"kidney\", \"liver\", \"muscle\", \"skin\"]\n",
    "df_cm = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sn.heatmap(df_cm, annot=True, fmt=\".3f\", square=True, cbar=False, cmap=\"Blues\", linewidths=3, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicted label\", labelpad=16)\n",
    "plt.ylabel(\"True label\", labelpad=12)\n",
    "plt.tick_params(axis='y', rotation=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
