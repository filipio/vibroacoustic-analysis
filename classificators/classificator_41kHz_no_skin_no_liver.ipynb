{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "import tensorflow.compat.v1 as tf \n",
    "from tensorflow.compat.v1 import keras\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_PATH = os.path.join(\"..\", \"animal_sounds\")\n",
    "DATASET_AUDIO_PATH = os.path.join(\"..\", \"animal_sounds_clips\")\n",
    "# The sampling rate to use.\n",
    "# This is the one used in all of the audio samples.\n",
    "# This will also be the output size of the audio wave samples\n",
    "# (since all samples are of 1 second long)\n",
    "SAMPLING_RATE = 44100\n",
    "# percentage of recordings used for validation\n",
    "TEST_RATIO = 0.15\n",
    "CLIP_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed to use when shuffling the dataset and the noise\n",
    "SHUFFLE_SEED = 5123\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "    return audio\n",
    "\n",
    "def audio_to_fft(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
    "    # we need to squeeze the dimensions and then expand them again\n",
    "    # after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT\n",
    "    # which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
    "\n",
    "\n",
    "# Get the list of audio file paths along with their corresponding labels\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "print(\"Our class names: {}\".format(class_names,))\n",
    "\n",
    "to_skip = [\"skin_animal\", \"skin_animal_valid\", \"liver_animal\", \"liver_animal_valid\"]\n",
    "\n",
    "train_audio_paths = []\n",
    "valid_audio_paths = []\n",
    "train_labels = []\n",
    "valid_labels = []\n",
    "for label, name in enumerate([c for c in class_names if c not in to_skip]):\n",
    "    print(\"Processing material {}\".format(name,))\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.lower().endswith(\".wav\")\n",
    "    ]\n",
    "    label = label // 2 # coz every dir has a _valid copy\n",
    "    if name.endswith(\"_valid\"):\n",
    "        valid_audio_paths += speaker_sample_paths\n",
    "        valid_labels += [label] * len(speaker_sample_paths)\n",
    "    else:\n",
    "        train_audio_paths += speaker_sample_paths\n",
    "        train_labels += [label] * len(speaker_sample_paths)\n",
    "    print(f\"Loaded {len(speaker_sample_paths)} files from class {label}.\")\n",
    "    \n",
    "print(\n",
    "    \"Found {} files belonging to {} classes.\".format(len(train_audio_paths) + len(valid_audio_paths), len(class_names)//2)\n",
    ")\n",
    "\n",
    "# Shuffle\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(train_audio_paths)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(valid_audio_paths)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(train_labels)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(valid_labels)\n",
    "\n",
    "print(\"Using {} files for training.\".format(len(train_labels)))\n",
    "print(\"Using {} files for validation.\".format(len(valid_labels)))\n",
    "\n",
    "# Create 2 datasets, one for training and the other for validation\n",
    "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
    "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
    "    # Shortcut\n",
    "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
    "    for i in range(conv_num - 1):\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = residual_block(inputs, 16, 2)\n",
    "    x = residual_block(x, 32, 2)\n",
    "    x = residual_block(x, 64, 3)\n",
    "\n",
    "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation=\"relu\", kernel_regularizer=keras.regularizers.L1L2(l1=1e-5, l2=1e-4))(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "model = build_model((SAMPLING_RATE // 2, 1), len(class_names))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model using Adam's default learning rate\n",
    "model.compile(\n",
    "    optimizer=\"Adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Add callbacks:\n",
    "# 'EarlyStopping' to stop training when the model is not enhancing anymore\n",
    "# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n",
    "model_save_filename = \"model_41kHz_no_skin_no_liver.h5\"\n",
    "\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)\n",
    "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    model_save_filename, monitor=\"accuraccy\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = Path(\"model_41kHz_no_skin_no_liver.h5\")\n",
    "history = None\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    model.load_weights(CHECKPOINT_PATH)\n",
    "else:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history is not None:\n",
    "    i = len(history.history[\"val_acc\"])-1\n",
    "    print(\"epoch\", i, \"\\nacc\", np.max(history.history[\"acc\"][i]), \"\\nval_acc\", history.history[\"val_acc\"][i])\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    axs[0].plot(history.history['acc'])\n",
    "    axs[0].plot(history.history['val_acc'])\n",
    "    axs[0].set_title('Model accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['training', 'validation'], loc='upper left')\n",
    "\n",
    "    axs[1].plot(history.history['loss'])\n",
    "    axs[1].plot(history.history['val_loss'])\n",
    "    axs[1].set_title('Model loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = paths_and_labels_to_dataset(train_audio_paths + valid_audio_paths, train_labels + valid_labels)\n",
    "test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "actual_class_names = [n for n in class_names if not n.endswith(\"_valid\") and n not in to_skip]\n",
    "\n",
    "confusion = np.zeros((len(actual_class_names),)*2, dtype=np.float32)\n",
    "counter = 0\n",
    "for audios, labels in test_ds.take(100):\n",
    "    # Get the signal FFT\n",
    "    ffts = audio_to_fft(audios)\n",
    "    # Predict\n",
    "    y_pred = model.predict(ffts)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    for idx, pred in enumerate(y_pred):\n",
    "        counter += 1\n",
    "        confusion[labels[idx], pred] += 1\n",
    "\n",
    "print(counter)\n",
    "diag_sum = np.sum(np.diag(confusion))\n",
    "print(f\"total={np.sum(confusion)}, on diagonal={diag_sum} totalAcc={diag_sum/np.sum(confusion)}\")\n",
    "\n",
    "for idx, label in enumerate(actual_class_names):\n",
    "    confusion[idx, :] /= np.sum(confusion[idx, :])\n",
    "    print(f\"{idx} - {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "labels = [x.split(\"_\")[0] for x in actual_class_names]\n",
    "# labels = [\"ribs\", \"kidney\", \"liver\", \"muscle\", \"skin\"]\n",
    "df_cm = pd.DataFrame(confusion, index=labels, columns=labels)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "sn.heatmap(df_cm, annot=True, fmt=\".3f\", square=True, cbar=False, cmap=\"Blues\", linewidths=3, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicted label\", labelpad=16)\n",
    "plt.ylabel(\"True label\", labelpad=12)\n",
    "plt.tick_params(axis='y', rotation=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
