{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.feature_extraction import EfficientFCParameters\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_PATH = os.path.join(\"..\", \"animal_sounds\")\n",
    "DATASET_AUDIO_PATH = os.path.join(\"..\", \"animal_sounds_clips\")\n",
    "SAMPLING_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of audio file paths along with their corresponding labels\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "print(\"Our class names: {}\".format(class_names,))\n",
    "\n",
    "train_audio_paths = []\n",
    "valid_audio_paths = []\n",
    "train_labels = []\n",
    "valid_labels = []\n",
    "for label, name in enumerate(class_names):\n",
    "    # print(\"Processing material {}\".format(name,))\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.lower().endswith(\".wav\")\n",
    "    ]\n",
    "    label = label // 2 # coz every dir has a _valid copy\n",
    "    if name.endswith(\"_valid\"):\n",
    "        valid_audio_paths += speaker_sample_paths\n",
    "        valid_labels += [label] * len(speaker_sample_paths)\n",
    "    else:\n",
    "        train_audio_paths += speaker_sample_paths\n",
    "        train_labels += [label] * len(speaker_sample_paths)\n",
    "    # print(f\"Loaded {len(speaker_sample_paths)} files from class {label}.\")\n",
    "    \n",
    "print(\n",
    "    \"Found {} files belonging to {} classes.\".format(len(train_audio_paths) + len(valid_audio_paths), len(class_names)//2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_audio(path):\n",
    "    y, _ = librosa.load(path, sr=SAMPLING_RATE)\n",
    "    return y\n",
    "\n",
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    audios = [path_to_audio(path) for path in audio_paths]\n",
    "    return np.array(audios), np.array(labels)\n",
    "\n",
    "def to_dataframe(dataset):\n",
    "    column_values = [i + 1 for i in range(len(dataset[0]))]\n",
    "    df = pd.DataFrame(data=dataset, columns=column_values)\n",
    "    df[\"id\"] = df.index\n",
    "    df = df.melt(id_vars=\"id\", var_name=\"time\").sort_values([\"id\", \"time\"]).reset_index(drop=True)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_labels = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
    "test_dataset, test_labels = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = to_dataframe(train_dataset)\n",
    "X_test = to_dataframe(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_take = 1_000_000\n",
    "\n",
    "X_train_features = extract_features(X_train[X_train['id'] < train_to_take], column_id=\"id\", column_sort=\"time\", impute_function=impute, default_fc_parameters=EfficientFCParameters())\n",
    "y_train = train_labels[:train_to_take]\n",
    "X_test_features = extract_features(X_test, column_id=\"id\", column_sort=\"time\", impute_function=impute, default_fc_parameters=EfficientFCParameters())\n",
    "y_test = test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = set()\n",
    "\n",
    "for label in np.unique(y_train):\n",
    "    y_train_binary = y_train == label\n",
    "    X_train_filtered = select_features(X_train_features, y_train_binary)\n",
    "    print(\"Number of relevant features for class {}: {}/{}\".format(label, X_train_filtered.shape[1], X_train_features.shape[1]))\n",
    "    relevant_features = relevant_features.union(set(X_train_filtered.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = X_train_features[list(relevant_features)]\n",
    "X_test_filtered = X_test_features[list(relevant_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered.columns = [i for i in range(X_train_filtered.shape[1])]\n",
    "X_test_filtered.columns = [i for i in range(X_test_filtered.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier()\n",
    "clf.fit(X_train_filtered, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy=accuracy_score(y_pred, y_test)\n",
    "print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = confusion_matrix(y_test, y_pred).astype(np.float64)\n",
    "print(cm, cm.shape)\n",
    "for i in range(cm.shape[0]):\n",
    "  cm[i, :] /= np.sum(cm[i, :])\n",
    "  \n",
    "cm_matrix = pd.DataFrame(data=cm)\n",
    "cm_matrix['types'] = np.array(['kidney','liver','muscle', 'ribs', 'skin'])\n",
    "\n",
    "cm_matrix.set_index('types', inplace=True)\n",
    "cm_matrix.columns = ['kidney','liver','muscle', 'ribs', 'skin']\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm_matrix, annot=True, fmt=\".3f\", square=True, cbar=False, cmap=\"Blues\", linewidths=3, vmin=0, vmax=1)\n",
    "plt.xlabel(\"Predicted label\", labelpad=16)\n",
    "plt.ylabel(\"True label\", labelpad=12)\n",
    "plt.tick_params(axis='y', rotation=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
